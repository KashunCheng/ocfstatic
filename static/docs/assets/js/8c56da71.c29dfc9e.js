"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[9893],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=u(n),d=r,h=c["".concat(s,".").concat(d)]||c[d]||m[d]||o;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=n[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},1483:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var a=n(7462),r=(n(7294),n(3905));const o={title:"Slurm"},i=void 0,l={unversionedId:"services/hpc/slurm",id:"services/hpc/slurm",title:"Slurm",description:"Slurm",source:"@site/docs/services/hpc/slurm.md",sourceDirName:"services/hpc",slug:"/services/hpc/slurm",permalink:"/docs/services/hpc/slurm",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/services/hpc/slurm.md",tags:[],version:"current",frontMatter:{title:"Slurm"},sidebar:"tutorialSidebar",previous:{title:"High performance computing",permalink:"/docs/services/hpc/"},next:{title:"Lab",permalink:"/docs/services/lab/"}},s={},u=[{value:"Slurm",id:"slurm",level:2},{value:"Running a job through Slurm",id:"running-a-job-through-slurm",level:2},{value:"When to use <code>srun</code> versus <code>sbatch</code>",id:"when-to-use-srun-versus-sbatch",level:4},{value:"Resource options",id:"resource-options",level:3},{value:"Using <code>srun</code>",id:"using-srun",level:3},{value:"Running an interactive terminal",id:"running-an-interactive-terminal",level:4},{value:"Using <code>sbatch</code>",id:"using-sbatch",level:3},{value:"Output from <code>sbatch</code>",id:"output-from-sbatch",level:4},{value:"Cancelling a job",id:"cancelling-a-job",level:4},{value:"Viewing Slurm info",id:"viewing-slurm-info",level:2}],p={toc:u};function m(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"slurm"},"Slurm"),(0,r.kt)("p",null,"In order to use the resources of our HPC computng nodes, you must submit your\ncomputing tasks through ",(0,r.kt)("a",{parentName:"p",href:"https://slurm.schedmd.com/"},"Slurm"),", which will ensure that your task, or\njob, is given exclusive access to some CPUs, memory, and GPUs if needed. Slurm\nalso intelligently queues jobs from different users to most efficiently use our\nnodes' resources."),(0,r.kt)("h2",{id:"running-a-job-through-slurm"},"Running a job through Slurm"),(0,r.kt)("p",null,"Slurm is only accessible while SSHed into ",(0,r.kt)("inlineCode",{parentName:"p"},"hpcctl.ocf.berkeley.edu"),"."),(0,r.kt)("p",null,"Submitting a job to Slurm can be done in one of two ways: through ",(0,r.kt)("inlineCode",{parentName:"p"},"srun"),", and\nthrough ",(0,r.kt)("inlineCode",{parentName:"p"},"sbatch"),"."),(0,r.kt)("p",null,"When using ",(0,r.kt)("inlineCode",{parentName:"p"},"srun"),", options are supplied using command-line flags, and the job\nis attached to the terminal you run ",(0,r.kt)("inlineCode",{parentName:"p"},"srun")," from. If there is a queue to run\njobs, the terminal will wait until your job starts running, and if the terminal\ncloses, the job will be cancelled."),(0,r.kt)("p",null,"By contrast, to submit a job using ",(0,r.kt)("inlineCode",{parentName:"p"},"sbatch"),", you must first create a batch file\nthat includes options for your job, and the commands that your job will run.\nThe batch file is then submitted, and after the job runs, log files with the\njob's output to ",(0,r.kt)("inlineCode",{parentName:"p"},"stdout")," are put into your home directory."),(0,r.kt)("h4",{id:"when-to-use-srun-versus-sbatch"},"When to use ",(0,r.kt)("inlineCode",{parentName:"h4"},"srun")," versus ",(0,r.kt)("inlineCode",{parentName:"h4"},"sbatch")),(0,r.kt)("p",null,"If your job requires interactivity or inputs from the terminal, or you need a\nterminal to test or experiment, use ",(0,r.kt)("inlineCode",{parentName:"p"},"srun"),". Otherwise, use ",(0,r.kt)("inlineCode",{parentName:"p"},"sbatch"),", as you\ndon't have to keep your terminal open until the job runs.\nAlternatively, you could use ",(0,r.kt)("inlineCode",{parentName:"p"},"screen")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"tmux")," with ",(0,r.kt)("inlineCode",{parentName:"p"},"srun")," to keep your\njob alive, even if you disconnect from your terminal."),(0,r.kt)("h3",{id:"resource-options"},"Resource options"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Some terminology:"),' Slurm refers to a process as a "task". Even if a single\nprocess is using multiple threads/CPUs, it still counts as one task.'),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"By default, without any flags, a job you submit will be allocated one CPU,\n100 MB of RAM, and no GPUs, and will run for at maximum 2 days. In order to\nallocate more resources and time to your job, you must set one or more of these\nflags:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"-n")," / ",(0,r.kt)("inlineCode",{parentName:"li"},"--ntasks"),":",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"The number of tasks/processes to allocate. Default is 1."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"-c")," / ",(0,r.kt)("inlineCode",{parentName:"li"},"--cpus-per-task"),":",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"The number of CPUs to allocate per task. Default is 1."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"--mem"),": - The total amount of RAM to allocate. By default, the number supplied is\nassumed to megabytes. However, the prefixes ",(0,r.kt)("inlineCode",{parentName:"li"},"K"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"M"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"G"),", and ",(0,r.kt)("inlineCode",{parentName:"li"},"T")," can be\nappended to the number instead. For example, to allocate 5 gigabytes of ram,\nuse ",(0,r.kt)("inlineCode",{parentName:"li"},"--mem=5G"),". Default is 100 megabytes."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"--gres")," ",(0,r.kt)("strong",{parentName:"li"},"(Optional)"),": - Allocates some GPUs to your job. The format is ",(0,r.kt)("inlineCode",{parentName:"li"},"--gres=gpu:[optional type]:[number to allocate]"),". For example, to allocate 2 GPUs of any type, you\nwould include ",(0,r.kt)("inlineCode",{parentName:"li"},"--gres=gpu:2"),". To allocate two Nvidia 1080Ti GPUs (our only type\nright now), you would include ",(0,r.kt)("inlineCode",{parentName:"li"},"--gres=gpu:nv1080:2"),". Default is no GPUs."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"--t")," / ",(0,r.kt)("inlineCode",{parentName:"li"},"--time")," ",(0,r.kt)("strong",{parentName:"li"},"(Optional)"),': - The maximum amount of time your job can take before Slurm forcefully\nkills it. Acceptable time formats include "minutes", "minutes:seconds",\n"hours:minutes:seconds", "days-hours", "days-hours:minutes" and\n"days-hours:minutes:seconds". You cannot set the time limit greater than the\ndefault, which is 2 days.')),(0,r.kt)("h3",{id:"using-srun"},"Using ",(0,r.kt)("inlineCode",{parentName:"h3"},"srun")),(0,r.kt)("p",null,"On ",(0,r.kt)("inlineCode",{parentName:"p"},"hpcctl.ocf.berkeley.edu"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"srun [command-line flags] [command to run]\n")),(0,r.kt)("p",null,"For example, to run a job that uses 4 CPUs, 8 GB of RAM, and 1 GPU:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'bzh@hpcctl:~$ srun --ntasks=1 --cpus-per-task=4 --mem=8G --gres=gpu:1 echo "Hello world!"\n\nHello world!\n')),(0,r.kt)("h4",{id:"running-an-interactive-terminal"},"Running an interactive terminal"),(0,r.kt)("p",null,"To start up an interactive terminal on a compute node, use the ",(0,r.kt)("inlineCode",{parentName:"p"},"--pty [your terminal of choice]")," flag. For most everyone, you'll be using ",(0,r.kt)("inlineCode",{parentName:"p"},"bash"),", so to\nstart an interactive terminal on a node, run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"srun [other command-line flags] --pty bash\n")),(0,r.kt)("h3",{id:"using-sbatch"},"Using ",(0,r.kt)("inlineCode",{parentName:"h3"},"sbatch")),(0,r.kt)("p",null,"A Slurm batch script is functionally the same as a regular ",(0,r.kt)("inlineCode",{parentName:"p"},"bash")," script: The\n",(0,r.kt)("inlineCode",{parentName:"p"},"bash")," shebang at the start, and script after."),(0,r.kt)("p",null,"However, to pass options into SLURM, you'll need to add some special comment\nlines, which are in the format ",(0,r.kt)("inlineCode",{parentName:"p"},"#SBATCH [command-line flag]=[value]"),". ",(0,r.kt)("strong",{parentName:"p"},"They\nmust be after the shebang but before any non-comments"),"."),(0,r.kt)("p",null,"For example, a batch script which uses 4 CPUs, 8 GB of RAM, and 1 GPU has its\ncontents as:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},' #!/bin/bash\n #SBATCH --ntasks=1\n #SBATCH --cpus-per-task=4\n #SBATCH --mem=8G\n #SBATCH --gres=gpu:1\n\n echo "Hello world!"\n')),(0,r.kt)("p",null,"You submit batch scripts to Slurm with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sbatch [path to batch script]\n")),(0,r.kt)("h4",{id:"output-from-sbatch"},"Output from ",(0,r.kt)("inlineCode",{parentName:"h4"},"sbatch")),(0,r.kt)("p",null,"By default, output from your job (",(0,r.kt)("inlineCode",{parentName:"p"},"stdout")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"stderr"),") is placed into a file\nin the directory you ran ",(0,r.kt)("inlineCode",{parentName:"p"},"sbatch")," from. it will be named ",(0,r.kt)("inlineCode",{parentName:"p"},"slurm-[your job's numeric ID].out"),"."),(0,r.kt)("p",null,"To specify a different output file, use the ",(0,r.kt)("inlineCode",{parentName:"p"},"-o")," / ",(0,r.kt)("inlineCode",{parentName:"p"},"--output")," flag. For\nexample, to redirect output to a file named ",(0,r.kt)("inlineCode",{parentName:"p"},"job.log")," in your home directory,\nuse ",(0,r.kt)("inlineCode",{parentName:"p"},"--output=~/job.log"),"."),(0,r.kt)("h4",{id:"cancelling-a-job"},"Cancelling a job"),(0,r.kt)("p",null,"To cancel your job before it's run, run ",(0,r.kt)("inlineCode",{parentName:"p"},"scancel [job ID]"),". Your job's ID is\noutput when a batch script is submitted, or you can find it using ",(0,r.kt)("inlineCode",{parentName:"p"},"squeue"),"\n(more details below)."),(0,r.kt)("h2",{id:"viewing-slurm-info"},"Viewing Slurm info"),(0,r.kt)("p",null,"To view the queue of running and pending jobs from all users, run ",(0,r.kt)("inlineCode",{parentName:"p"},"squeue"),". To\nsee the details of one job, run ",(0,r.kt)("inlineCode",{parentName:"p"},"squeue -j [job ID]"),"."),(0,r.kt)("p",null,"To view the list of all HPC nodes, and some details about them, run ",(0,r.kt)("inlineCode",{parentName:"p"},"sinfo -N -l"),"."))}m.isMDXComponent=!0}}]);